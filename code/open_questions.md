# Open Questions Week 5 (Main)
Let's suppose that the data is partitioned by visit_id in our Apache Kafka topic. My questions
are:
- Do you need to use groupBy in your data processing logic to implement the
sessionization pipeline?

**Answer:** No, in this case we don't need the `groupByKey()` method.
Instead of creating a tuple, the `extract_columns_for_session_creation()` function would in this case only create the dictionary containing the values necessary to create the session.
Furthermore, the `session_creation()` function would then need to loop over all rdd-rows to create the session.

- If you base your processing logic per partition basis (no group by in the code), what
change in your topic topology (replication factor, partitions, something else) can break
your processing logic?

**Answer:**
We would need to create a new partition for each new `visit_id`.
This is however not a fixed but an ever-growing number (assuming that the same user will get a new visit_id for every time he visits the website).
There are two limiting factors:
As stated in this [confluent blog post](https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/)
the overhead generated by the partitions requires more memory.
This is a soft limiting factor as we can always add more memory - although there is a limit where it just doesn't make sense anymore.
The second hard limiting factor is mentioned in [Apache blog post](https://blogs.apache.org/kafka/entry/apache-kafka-supports-more-partitions):
According to their findings, the limits are 4.000 partitions per topic or 200.000 partitions for the cluster in order to stay within a reasonable range of performance.

- What are the dangers of partitioning records by visit_id?

**Answer:** While the number of partitions is ever-growing with each new `visit_id`,
there is no data added to existing partitions as soon as the exit-condition of the session is met (assuming that a user will get a new `visit_id` with his next visit).
Thus, we will end up with a highly inefficient architecture as we do not use the parallel processing property of partitions.
Furthermore, our architecture is bound to fail as soon as the limit of partitions is reached.
To avoid this, we would have to manually manage the pipeline and clean up "dead" partitions.
However, this again leeds to new issues.

# Open Questions Week 5 (Bonus)

- Let's suppose that you have just joined a team, and you got a task to fix a sessionization
application. The application is running on production, it doesn't have unit tests, the logs don't
allow you to identify the errors, and the errors are located in the business logic. Your debugging
process cannot impact the production application, so you cannot shut it down and debug.
Please explain what will be your debugging process? What will be your first step, what you will
do next? Explain why you chose these steps.

**Answer:** Steps:
1. **Add a new consumer:**
First I would add a new consumer with the code version of the consumer running in production but this consumer will not send the processed data to other applications.
Thus, we can test and debug this consumer without effecting the business processes.
2. **Reproduce the issue with the sessionization:**
Before doing anything else, I would go sure that the consumer is actually needs to fixed.
If not, then the error must be somewhere else.
3. **Debug the new consumer:** Now that we know the error is within this new consumer, we can stop and debug it.
4. **Write a fix:** After finding the error we can write a fixed version of the consumer.
5. **Test the fix:** Before deploying the fix on production we need to test the new code version.
6. **Deploy the fix:** Depending on the data architecture, we now can either:
   7. Deploy the new fix to the buggy consumer and reprocess the data
   8. Deploy a new consumer with the fixed version, stop the buggy consumer and potentially reprocess the data with another batch pipeline.




- In the previous exercise we generated the sessions only for 1 partition-window. Remember,
we're synchronizing data from Kafka to our file system and put it into hourly partitions based on
the `event_time` field.
The problem with our code is that we can generate only partially valid sessions. For example, if
one visit starts at 9:30 and terminates at 10:40, we'll generate it into 2 files, one generated in the
batch for 9 o'clock, and one in the batch for 10 o'clock.
Please propose a solution to generate the session only in the last processing window (10
o'clock for our example). Choose the format you're the most comfortable with, it can be a
graphical representation, textual explanation, pseudo-code â€¦ .

**Answer:**
My first approach would be to deliver the `visit_id` as a key-value from the producer to the kafka-topic.
Thus, the partition would be a hashed value of the key-value.
Hence, we could be sure, that all the data necessary for creating a session is added to the same partition.
Then we would only need to have some kind of timer or condition that checks whether the `event_time` of the last event belonging to the same session
is old enough so that the session can be seen as expired - or the event holds the exiting action.

However, assuming that the partitions need to be based `event_time`:
We can relocate all data that is not expired or posses an exit condition from one partition to the other.
Thus, the consumer moves all events that cannot be put into a complete session to the next partition.

If this cannot be done my next suggestion would be to give the consumer 2 data sources:
First, of course the events held in the partition that should be processed.
The second data source is the consumer that holds incomplete sessions.
If it is the same consumer, it only needs to keep a hold on all partitions that could not be processed in the previous time window.
